{"cells":[{"metadata":{"id":"hfp1nArSBTaT","colab_type":"text"},"cell_type":"markdown","source":"**Project Name : Real or Not? NLP with Disaster Tweets**\n\nIn this competition, we’re challenged to build a machine learning model that predicts which Tweets are about real disasters and which one’s aren’t.\n\nPrize Money : 10 000$\n\nAuthors : Merhbene Oumaima AND Azzouz Myriam "},{"metadata":{"id":"BEamlTk_BTaV","colab_type":"text"},"cell_type":"markdown","source":"**References**\n\n* Source for bert_encode function: https://www.kaggle.com/user123454321/bert-starter-inference\n* All pre-trained BERT models from Tensorflow Hub: https://tfhub.dev/s?q=bert\n* https://github.com/google-research/bert#learning-a-new-wordpiece-vocabulary\n\n**Kernels that inspired us ** \n* https://www.kaggle.com/cabonfim10/basic-nlp-disaster-tweets\n* https://www.kaggle.com/vbmokin/nlp-eda-bag-of-words-tf-idf-glove-bert\n* https://www.kaggle.com/xhlulu/disaster-nlp-keras-bert-using-tfhub\n"},{"metadata":{"id":"FD7JI9f73rCd","colab_type":"text"},"cell_type":"markdown","source":"\nIn this Notebook two approches were introduced . The first approach uses Neural Networks and the second approach uses Bert : Bidirectional Encoder Representations from Transformers wich uses a pretrained model on Tensorflow Hub . As there are two Bert models : base and large , in our project we will be using the base model : BERT Base: 12 layers (transformer blocks), 12 attention heads, and 110 million parameters . As a conclusion we will compare both approaches .\n\nPlan : \n\n1/ NLP using Neural Networks\n\n* Data Visualization\n\n* Data Preprocessing\n\n* Model Building\n\n*   Submission\n\n\n\n\n\n\n\n\n2/ NLP using BERT\n\n\n* Bert input Encoding\n\n* Bert model \n\n3/Conclusion \n\n   \n\n---\n\n\n"},{"metadata":{"id":"75iGjRznhI_o","colab_type":"text"},"cell_type":"markdown","source":"**Library Load**\n"},{"metadata":{"trusted":true,"id":"IaSHqbgC3rCf","colab_type":"code","colab":{}},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re\nimport string\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense ,Activation, Dropout, BatchNormalization\nfrom keras.optimizers import Adam\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"id":"01Q2138x3rCj","colab_type":"code","outputId":"d7f658ac-2b9c-4f5f-97d8-1d3ccafe4a7e","colab":{}},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":0,"outputs":[]},{"metadata":{"id":"N1Cq-bCkhI_v","colab_type":"text"},"cell_type":"markdown","source":"**Data Load**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"id":"Q6cGaAlc3rCm","colab_type":"code","colab":{}},"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\nsample_submission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")","execution_count":0,"outputs":[]},{"metadata":{"id":"wQO651ZlUKDT","colab_type":"code","trusted":true,"colab":{},"outputId":"32c22634-8d56-4607-f41a-6ea95c18d87d"},"cell_type":"code","source":"train_df.head(3)","execution_count":0,"outputs":[]},{"metadata":{"id":"JdQmh-NwUu2L","colab_type":"code","trusted":true,"colab":{},"outputId":"f5c35e47-de1a-4178-c3f5-e5d390ad9c35"},"cell_type":"code","source":"test_df.head(3)","execution_count":0,"outputs":[]},{"metadata":{"id":"U1c0TiNtUeAk","colab_type":"code","trusted":true,"colab":{},"outputId":"633aa4ce-aecb-46af-d551-0a7e6dc2579c"},"cell_type":"code","source":"print('There are {} rows and {} columns in train'.format(train_df.shape[0],train_df.shape[1]))\nprint('There are {} rows and {} columns in test'.format(test_df.shape[0],test_df.shape[1]))","execution_count":0,"outputs":[]},{"metadata":{"id":"L9qNYdOYBTag","colab_type":"text"},"cell_type":"markdown","source":"\n\n\n\n\n# 1/ NLP using Neural Networks "},{"metadata":{"id":"8G06qqeV3rCo","colab_type":"text"},"cell_type":"markdown","source":"* **Data Visualization:**\n\n\n\n---\n\n\n\n\n\nLet's do some exploration and visualization of the data. This will help us to gain some valuable insights about the dataset."},{"metadata":{"trusted":true,"id":"osis8oqx3rCo","colab_type":"code","outputId":"bedb598d-b011-48d3-9f9b-263739b23048","colab":{}},"cell_type":"code","source":"import seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfig, axes = plt.subplots(ncols=2, figsize=(17, 4), dpi=100)\nplt.tight_layout()\n\nlabels=['Disaster Tweet','No Disaster']\nsize=  [train_df['target'].mean()*100,abs(1-train_df['target'].mean())*100]\nexplode = (0, 0.1)\n#ig1,ax1 = plt.subplots()\naxes[0].pie(size,labels=labels,explode=explode,shadow=True,\n            startangle=90,autopct='%1.1f%%')\nsns.countplot(x=train_df['target'], hue=train_df['target'], ax=axes[1])\nplt.show()","execution_count":0,"outputs":[]},{"metadata":{"id":"NjqxkxIG3rCr","colab_type":"text"},"cell_type":"markdown","source":"There are only two classes 0 and 1.\nThe dataset is balanced. And we can also understand that tweets about disaster are less than those about no disaster."},{"metadata":{"id":"09rMPbsA3rCs","colab_type":"text"},"cell_type":"markdown","source":"* **Data Preprocessing**\n\n\n---\n\n\n\nTweets always have to be cleaned before we go onto modelling.So we will do some basic cleaning such as removing punctuations,removing html tags and emojis ect ..\n\n\n\nOur end goal is to have our input as a sentence using only keywords, we can then encode those words and try to find patterns in them later on.\n"},{"metadata":{"id":"1yd0wh8eYlmb","colab_type":"text"},"cell_type":"markdown","source":"***A- Cleaning Data :***"},{"metadata":{"id":"rH_5zaK7BTal","colab_type":"text"},"cell_type":"markdown","source":"**1-Removing URLS and emoji:**\n\n\n\n\nFirst let's get rid of any website URLs in the tweets"},{"metadata":{"trusted":true,"id":"fok8KKX63rCt","colab_type":"code","colab":{}},"cell_type":"code","source":"def remove_url(text):\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url.sub(r'',text)\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)","execution_count":0,"outputs":[]},{"metadata":{"id":"n3dsu9QC3rCz","colab_type":"text"},"cell_type":"markdown","source":"**2-Removing punctuation:**\n\nNext we want to remove any punctuation, especially important considering the fact that this will remove the '@' and '#' symbols that are so common on twitter."},{"metadata":{"trusted":true,"id":"6I95N0233rC0","colab_type":"code","colab":{}},"cell_type":"code","source":"def remove_punc(text):\n    return text.translate(str.maketrans('', '', string.punctuation))    ","execution_count":0,"outputs":[]},{"metadata":{"id":"znAHq9Ax3rC2","colab_type":"text"},"cell_type":"markdown","source":"**3-Tokenization:**\n\nAt this point we've removed a lot of the fluff that goes with tweets, now let's tokenize them. This is a common form of lexical analysis which will split the tweets into its individual components (words/sentences/etc)"},{"metadata":{"trusted":true,"id":"3Y40lNmM3rC3","colab_type":"code","colab":{}},"cell_type":"code","source":"def tokenization(text):\n    text = re.split('\\W+', text)\n    return text","execution_count":0,"outputs":[]},{"metadata":{"id":"xjPTFXcc3rC5","colab_type":"text"},"cell_type":"markdown","source":"**4-Removing Stop words:**\n\nNext step is to remove the stopwords. These are the standard words like 'and', \n'the', 'a' and so on. These are so prevalent in language that they'll cloud our data. We only want to deal with keywords so we'll get rid of these stopwords\n\n"},{"metadata":{"trusted":true,"id":"P2fe3GYt3rC5","colab_type":"code","colab":{}},"cell_type":"code","source":"import nltk\nfrom nltk.tokenize import word_tokenize\nstopword = nltk.corpus.stopwords.words('english')\ndef remove_stopwords(text):\n    text = [word for word in text if word not in stopword]\n    return text","execution_count":0,"outputs":[]},{"metadata":{"id":"ShieFKEG3rC8","colab_type":"text"},"cell_type":"markdown","source":"Now it's time to apply these functions to our dataset, the result will be a much cleaner set of tweets. This will help a lot when we try to build a model later on\n\n"},{"metadata":{"trusted":true,"id":"ZCxXtWi_3rC9","colab_type":"code","colab":{}},"cell_type":"code","source":"for datas in [train_df,test_df]:\n    datas['text'] = datas['text'].apply(lambda x : remove_url(x))\n    datas['text'] = datas['text'].apply(lambda x : remove_emoji(x))\n    datas['text'] = datas['text'].apply(lambda x : remove_punc(x))\n    datas['text'] = datas['text'].apply(lambda x : tokenization(x.lower()))\n    datas['text'] = datas['text'].apply(lambda x : remove_stopwords(x))\n    datas['text'] = datas['text'].apply(lambda x : ' '.join(x))","execution_count":0,"outputs":[]},{"metadata":{"id":"fJLHMUp63rDA","colab_type":"text"},"cell_type":"markdown","source":"**B-Tweets Encoding**\n\n\n\n\n\nIt's a way to turn our tweets into numbers. One good way is by vectorising them. This will maintain the relationship between similar words/phrases and will allow our model to find patterns in the data. "},{"metadata":{"trusted":true,"id":"cTUeue-f3rDA","colab_type":"code","colab":{}},"cell_type":"code","source":"count_vec = CountVectorizer()\ntrain_vec = count_vec.fit_transform(train_df['text'])\ntest_vec = count_vec.transform(test_df['text'])","execution_count":0,"outputs":[]},{"metadata":{"trusted":true,"id":"_HlywhjE3rDD","colab_type":"code","outputId":"decd8956-40ed-4121-cd8b-41dc3e9120b8","colab":{}},"cell_type":"code","source":"print(train_vec.shape)\nprint(train_vec.toarray())","execution_count":0,"outputs":[]},{"metadata":{"id":"PeQ_mYiS3rDF","colab_type":"text"},"cell_type":"markdown","source":"\n\n*   **Model Building:**\n\n\n\n\n---\n\n\n\n\nNext we will build the actual model. We will use the neural network ,the key tool of machine learning. To reduce the overfitting in neural networks , we well use the regularization technique Dropout. In addition , to improve the speed, performance, and stability we will use Batch normalization .\n"},{"metadata":{"trusted":true,"id":"cdvPCTTA3rDG","colab_type":"code","outputId":"a516c110-691f-4acf-841d-2c68bdb5dcec","colab":{}},"cell_type":"code","source":"\nmodel = Sequential()\n\nmodel.add(Dense(1024, activation='relu', input_shape=(17667,)))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(512, activation='relu', input_dim=512))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()\n\n\n","execution_count":0,"outputs":[]},{"metadata":{"id":"I8CGMgawZ2ux","colab_type":"code","trusted":true,"colab":{},"outputId":"4d43e69e-0763-4c64-d767-d4a5fea8ed76"},"cell_type":"code","source":"x_train=train_vec\ny_train=train_df[\"target\"]\nx_test=test_vec\n\n          \nx_val = x_train[:1000]\npartial_x_train = x_train[1000:]\ny_val = y_train[:1000]\npartial_y_train = y_train[1000:]          \n\nhistory = model.fit (partial_x_train, partial_y_train,verbose=1,epochs=20,batch_size=32, validation_data=(x_val, y_val))","execution_count":0,"outputs":[]},{"metadata":{"id":"0yqqgnJ13rDI","colab_type":"text"},"cell_type":"markdown","source":"\n\n\n\n\n*   **Submission:** \n\n\n---\n\n\n\n\n\n\nFinally we take our predictions and upload them to the competition. The final score was 0.779, which isn't too bad considering the naivety of this implementation. "},{"metadata":{"trusted":true,"id":"WwozyFRc3rDJ","colab_type":"code","colab":{},"outputId":"38bf2027-1e6b-4ab0-88bc-e3eeb4576891"},"cell_type":"code","source":"sample_submission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\ntest_pred = model.predict(x_test)\nsample_submission['target'] = test_pred.round().astype(int)\nsample_submission.head()\n","execution_count":0,"outputs":[]},{"metadata":{"trusted":true,"id":"-7PrlUjU3rDL","colab_type":"code","colab":{}},"cell_type":"code","source":"sample_submission.to_csv(\"submission.csv\", index=False)","execution_count":0,"outputs":[]},{"metadata":{"id":"OisD1YPOBTbA","colab_type":"text"},"cell_type":"markdown","source":"# 2/ **NLP using BerT**"},{"metadata":{"id":"nPP8_ROuBTbB","colab_type":"text"},"cell_type":"markdown","source":"BERT is a method of pre-training language representations, meaning that we train a general-purpose \"language understanding\" model on a large text corpus (like Wikipedia), and then use that model for downstream NLP tasks that we care about (like question answering). BERT outperforms previous methods because it is the first unsupervised, deeply bidirectional system for pre-training NLP.\n"},{"metadata":{"trusted":true,"id":"h71Oiw12BTbC","colab_type":"code","colab":{},"outputId":"e9f28c7d-88fe-4105-c739-c79bd9e8fe07"},"cell_type":"code","source":"!pip install bert-tensorflow","execution_count":0,"outputs":[]},{"metadata":{"id":"9CasUMX8BTbE","colab_type":"text"},"cell_type":"markdown","source":"Official tokenization script created by the Google team"},{"metadata":{"trusted":true,"id":"G4wK8VGfBTbE","colab_type":"code","colab":{},"outputId":"33caaf5f-d1c0-49bf-d9f8-7544e80b22e8"},"cell_type":"code","source":"!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py","execution_count":0,"outputs":[]},{"metadata":{"id":"qBmCcZtHFt6_","colab_type":"code","colab":{},"trusted":false},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\nimport tokenization","execution_count":0,"outputs":[]},{"metadata":{"id":"iAw_3ceCBTbN","colab_type":"text"},"cell_type":"markdown","source":"BERT uses three embeddings to compute the input representations. They are :\n\n-token embeddings\n\n-segment embeddings\n\n-position embeddings.\n\n“CLS” is the reserved token to represent the start of sequence while “SEP” separate segment (or sentence).\n[CLS] : The first token of every sequence. A classification token which is normally used in conjunction with a softmax layer for classification tasks. For anything else, it can be safely ignored."},{"metadata":{"id":"k9_078oSBTbO","colab_type":"text"},"cell_type":"markdown","source":"In this function : 'bert_encode' max_len=512 , it means that we consider a tweet sentence having at max 512 word.\nWe can use up to 512, but we can use shorter if possible for memory and speed reasons."},{"metadata":{"trusted":true,"id":"Y96M28KIBTbO","colab_type":"code","colab":{}},"cell_type":"code","source":"def bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n        print('text',text)\n            \n        text = text[:max_len-2]\n        #“CLS” is the reserved token to represent the start of sequence\n        #“SEP” separate segment (or sentence)\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"] \n        #regrioupement : padding ramener tous les inputs à une meme longueur pour tre traités par Bert \n        pad_len = max_len - len(input_sequence)\n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        print('tokens',tokens)\n        #regroupement : padding ramener tous les inputs à une meme longueur pour tre traités par Bert\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        print('pad',pad_masks)\n        segment_ids = [0] * max_len\n        print('segment_ids',segment_ids)\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","execution_count":0,"outputs":[]},{"metadata":{"id":"yXoQXzL9BTbQ","colab_type":"text"},"cell_type":"markdown","source":"Once trained, the Bert model can provide several tasks ranging from binary classification of a text to answering questions and translating.\n\nWe specify the type of output in sequence_output [:, 0 ,:] to say that we want all the sequences entered at the first position [CLS]: classification and all the hidden layers.\n\nFinally we activate sigmoid in order to return a number between 0 and 1."},{"metadata":{"trusted":true,"id":"8plXz50WBTbR","colab_type":"code","colab":{}},"cell_type":"code","source":"def build_model(bert_layer, max_len=512):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    print(sequence_output) # sequence_output shape is (batch_size, max_len, hidden_dim)\n    clf_output = sequence_output[:, 0, :]\n    print('clf : ',clf_output)\n    out = Dense(1, activation='sigmoid')(clf_output) \n    print('out : ',out)\n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    model.compile(Adam(lr=2e-5), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","execution_count":0,"outputs":[]},{"metadata":{"id":"xEyom23CBTbU","colab_type":"text"},"cell_type":"markdown","source":"### Load BERT from TFHub"},{"metadata":{"id":"Wyyado4GBTbU","colab_type":"text"},"cell_type":"markdown","source":"* Load BERT from the Tensorflow Hub\n"},{"metadata":{"trusted":true,"id":"n83CtzDpBTbV","colab_type":"code","colab":{},"outputId":"904650a4-3b6a-480d-ecd5-98d9efec596b"},"cell_type":"code","source":"module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\"\nbert_layer = hub.KerasLayer(module_url, trainable=True)\n","execution_count":0,"outputs":[]},{"metadata":{"id":"Q1czy8YQBTbX","colab_type":"text"},"cell_type":"markdown","source":"\n* Load tokenizer from the bert layer\n"},{"metadata":{"id":"nFq5SzfTBTbX","colab_type":"text"},"cell_type":"markdown","source":"A vocab file (vocab.txt) is used to map WordPiece to word id."},{"metadata":{"trusted":true,"id":"IVsgEVTtBTbY","colab_type":"code","colab":{}},"cell_type":"code","source":"vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","execution_count":0,"outputs":[]},{"metadata":{"id":"kBVp1YQKBTba","colab_type":"text"},"cell_type":"markdown","source":"* Encode the text into tokens, masks, and segment flags"},{"metadata":{"trusted":true,"id":"4l0JQqN1BTba","colab_type":"code","colab":{}},"cell_type":"code","source":"train_input = bert_encode(train_df.text.values, tokenizer, max_len=160)\ntest_input = bert_encode(test_df.text.values, tokenizer, max_len=160)\ntrain_labels = train_df.target.values","execution_count":0,"outputs":[]},{"metadata":{"trusted":true,"id":"QCko7VwPBTbd","colab_type":"code","colab":{}},"cell_type":"code","source":"model = build_model(bert_layer, max_len=160)\nmodel.summary()\n","execution_count":0,"outputs":[]},{"metadata":{"id":"_Cq0bRAsBTbh","colab_type":"text"},"cell_type":"markdown","source":"In this model we could also use a validation split."},{"metadata":{"trusted":true,"id":"TN2Mx-OhBTbl","colab_type":"code","colab":{}},"cell_type":"code","source":"checkpoint = ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True)\n\ntrain_history = model.fit(\n    train_input, train_labels,\n    epochs=3,\n    callbacks=[checkpoint],\n    batch_size=64\n) ","execution_count":0,"outputs":[]},{"metadata":{"trusted":true,"id":"-9210EXIBTbo","colab_type":"code","colab":{}},"cell_type":"code","source":"test_pred = model.predict(test_input)","execution_count":0,"outputs":[]},{"metadata":{"trusted":true,"id":"ftLs9g12BTbp","colab_type":"code","colab":{}},"cell_type":"code","source":"sample_submission['target'] = test_pred.round().astype(int)\nsample_submission.to_csv('submission2.csv', index=False)","execution_count":0,"outputs":[]},{"metadata":{"id":"xmAD6I8VBTbr","colab_type":"text"},"cell_type":"markdown","source":"# 3/ Conclusion \n\nIn conclusion the neural networks model gives us an accuracy of 0.779 and we were ranked 927 , while Bert approach gives an accuracy of 0.833 and our actual ranking is 293/1338 .\nBert official paper developed by google and  Bert tutorials we had mentioned on top of our notebook helped us a lot get into the transfer learning used in NLP that helps gain time training  and efficiency . "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"colab":{"name":"Projet finale DL.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":1}